{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Глубинное обучение для текстовых данных, ФКН ВШЭ\n",
    "\n",
    "## Домашнее задание 1: Text Suggestion\n",
    "\n",
    "### Оценивание и штрафы\n",
    "\n",
    "Максимально допустимая оценка за работу — 10 баллов. Сдавать задание после жесткого дедлайна нельзя. При сдачи решения после мягкого дедлайна за каждый день просрочки снимается по одному баллу.\n",
    "\n",
    "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов. Весь код должен быть написан самостоятельно. Чужим кодом для пользоваться запрещается даже с указанием ссылки на источник. В разумных рамках, конечно. Взять пару очевидных строчек кода для реализации какого-то небольшого функционала можно.\n",
    "\n",
    "Неэффективная реализация кода может негативно отразиться на оценке. Также оценка может быть снижена за плохо читаемый код. Все ответы должны сопровождаться кодом или комментариями о том, как они были получены.\n",
    "\n",
    "__Мягкий дедлайн: 24 нояб\n",
    "\n",
    "__Жесткий дедлайн: 27 нояб\n",
    "\n",
    "\n",
    "### О задании\n",
    "\n",
    "В этом задании вам предстоит реализовать систему, предлагающую удачное продолжение слова или нескольких следующих слов в режиме реального времени по типу тех, которые используются в телефонах, поисковой строке или приложении почты. Полученную систему вам нужно будет обернуть в пользовательский интерфейс с помощью библиотеки [reflex](https://github.com/reflex-dev/reflex), чтобы ей можно было удобно пользоваться, а так же, чтобы убедиться, что все работает как надо. В этот раз вам не придется обучать никаких моделей, мы ограничимся n-граммной генерацией.\n",
    "\n",
    "### Структура\n",
    "\n",
    "Это домашнее задание состоит из двух частей предположительно одинаковых по сложности. В первой вам нужно будет выполнить 5 заданий, по итогам которых вы получите минимально рабочее решение. А во второй, пользуясь тем, что вы уже сделали реализовать полноценную систему подсказки текста с пользовательским интерфейсом. Во второй части мы никак не будем ограничивать вашу фантазию. Делайте что угодно, лишь бы получилось в результате получился удобный фреймворк. Чем лучше у вас будет результат, тем больше баллов вы получите. Если будет совсем хорошо, то мы добавим бонусов сверху по своему усмотрению.\n",
    "\n",
    "### Оценивание\n",
    "При сдаче зададания в anytask вам будет необходимо сдать весь код, а также отчет с подробным описанием техник, которые в применили для создания вашей системы. Не лишним будет также написать и о том, что у вас не получилось и почему.\n",
    "\n",
    "За часть с заданиями можно будет получить до __5__ баллов, за отчет – до __3__ баллов, 2 балл за доп вопросы, если возникнут, если вопросов не возникло, считаем, что 2 балла вы получили "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Данные\n",
    "\n",
    "Для получения текстовых статистик используйте датасет `emails.csv`. Вы можете найти его по [ссылке](https://disk.yandex.ru/d/ikyUhWPlvfXxCg). Он содержит более 500 тысяч электронных писем на английском языке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mДля выполнения ячеек с \"venv\" требуется пакет ipykernel.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/tatyanagordenko/Desktop/ВШЭ/NLP/HW_1/text_suggestion_app/venv/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "emails = pd.read_csv('emails.csv')\n",
    "len(emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметьте, что данные очень грязные. В каждом письме содержится различная мета-информация, которая будет только мешать при предсказании продолжения текста.\n",
    "\n",
    "__Задание 1 (1 балл).__ Очистите корпус текстов по вашему усмотрению. В идеале обработанные тексты должны содержать только текст самого письма и ничего лишнего по типу ссылок, адресатов и прочих символов, которыми мы точно не хотим продолжать текст. Оценка будет выставляться по близости вашего результата к этому идеалу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/tatyanagordenko/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "\n",
    "nltk.download('words')\n",
    "word_list = set(words.words())\n",
    "\n",
    "def clean_message(msg):\n",
    "    msg = re.sub(r'[^\\w\\s]', '', msg)\n",
    "    tokens = msg.split()\n",
    "    filtered_words = [word for word in tokens if word.lower() in word_list]\n",
    "    return filtered_words\n",
    "    #return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails['cleaned_message'] = emails['message'].apply(clean_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для следующего задания вам нужно будет токенизировать текст. Для этого просто разбейте его по словам. Очевидно, итоговый результат будет лучше, если ваша система также будет предлагать уместную пунктуацию. Но если вы считаете, что результат получается лучше без нее, то можете удалить все небуквенные символы на этапе токенизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [Date, Mon, May, From, To, Subject, K, Mail, H...\n",
       "1         [Date, May, From, To, Subject, Re, K, J, J, Ma...\n",
       "2         [Date, Wed, From, To, Subject, Re, test, K, Va...\n",
       "3         [Date, Mon, From, To, Subject, K, L, Gay, mail...\n",
       "4         [Date, From, To, Subject, Re, Hello, K, Piper,...\n",
       "                                ...                        \n",
       "517396    [Date, Wed, PST, From, To, Subject, Trade, wit...\n",
       "517397    [Date, Wed, PST, From, To, Subject, Gas, Some,...\n",
       "517398    [Date, Wed, PST, From, To, Subject, RE, CONFID...\n",
       "517399    [Date, Tue, PST, From, To, Subject, Slone, Ana...\n",
       "517400    [Date, Mon, PST, From, To, Subject, RE, i, thi...\n",
       "Name: cleaned_message, Length: 517401, dtype: object"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails['cleaned_message']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнение слова\n",
    "\n",
    "Описанная система будет состоять из двух частей: дополнение слова до целого и генерация продолжения текста (или вариантов продолжений). Начнем с первой части.\n",
    "\n",
    "В этой части вам предстоит реализовать метод дополнения слова до целого по его началу (префиксу). Для этого сперва необходимо научиться находить все слова, имеющие определенный префикс. Мы будем вызывать функцию поиска подходящих слов после каждой напечатанной пользователем буквы. Поэтому нам очень важно, чтобы поиск работал как можно быстрее. Простой перебор всех слов занимает $O(|V| \\cdot n)$ времени, где $|V|$ – размер словаря, а $n$ – длина префикса. Мы же напишем [префиксное дерево](https://ru.wikipedia.org/wiki/Префиксное_дерево), которое позволяет искать слова за $O(n + m)$, где $m$ – число подходящих слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 2 (1 балл).__ Допишите префиксное дерево для поиска слов по префиксу. Ваше дерево должно работать за $O(n + m)$ операции, в противном случае вы не получите баллов за это задание."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class PrefixTreeNode:\n",
    "    def __init__(self):\n",
    "        # словарь с буквами, которые могут идти после данной вершины\n",
    "        self.children: dict[str, PrefixTreeNode] = {}\n",
    "        self.is_end_of_word = False\n",
    "\n",
    "class PrefixTree:\n",
    "    def __init__(self, vocabulary: List[str]):\n",
    "        \"\"\"\n",
    "        vocabulary: список всех уникальных токенов в корпусе\n",
    "        \"\"\"\n",
    "        self.root = PrefixTreeNode()\n",
    "        for word in vocabulary:\n",
    "            self.insert(word)\n",
    "        \n",
    "    def insert(self, word: str):\n",
    "        node = self.root\n",
    "        for char in word:\n",
    "            if char not in node.children:\n",
    "                node.children[char] = PrefixTreeNode()\n",
    "            node = node.children[char]\n",
    "        node.is_end_of_word = True\n",
    "\n",
    "    def search_prefix(self, prefix) -> List[str]:\n",
    "        \"\"\"\n",
    "        Возвращает все слова, начинающиеся на prefix\n",
    "        prefix: str – префикс слова\n",
    "        \"\"\"\n",
    "\n",
    "        node = self.root\n",
    "        for char in prefix:\n",
    "            if char in node.children:\n",
    "                node = node.children[char]\n",
    "            else:\n",
    "                return []\n",
    "\n",
    "        results = []\n",
    "        self._dfs(node, prefix, results)\n",
    "        return results\n",
    "\n",
    "    def _dfs(self, node: PrefixTreeNode, prefix: str, results: List[str]):\n",
    "        if node.is_end_of_word:\n",
    "            results.append(prefix)\n",
    "        for char, child_node in node.children.items():\n",
    "            self._dfs(child_node, prefix + char, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = ['aa', 'aaa', 'abb', 'bba', 'bbb', 'bcd']\n",
    "prefix_tree = PrefixTree(vocabulary)\n",
    "\n",
    "assert set(prefix_tree.search_prefix('a')) == set(['aa', 'aaa', 'abb'])\n",
    "assert set(prefix_tree.search_prefix('bb')) == set(['bba', 'bbb'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда у нас есть способ быстро находить все слова с определенным префиксом, нам нужно их упорядочить по вероятности, чтобы выбирать лучшее. Будем оценивать вероятность слова по частоте его встречаемости в корпусе.\n",
    "\n",
    "__Задание 3 (1 балл).__ Допишите класс `WordCompletor`, который формирует словарь и префиксное дерево, а так же умеет находить все возможные продолжения слова вместе с их вероятностями. В этом классе вы можете при необходимости дополнительно отфильтровать слова, например, удалив все самые редкие. Постарайтесь максимально оптимизировать ваш код."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from collections import defaultdict\n",
    "\n",
    "class WordCompletor:\n",
    "    def __init__(self, corpus):\n",
    "        \"\"\"\n",
    "        corpus: list – корпус текстов\n",
    "        \"\"\"\n",
    "        self.word_counts = defaultdict(int)\n",
    "        self.total_words = 0\n",
    "        for document in corpus:\n",
    "            for word in document:\n",
    "                self.word_counts[word] += 1\n",
    "                self.total_words += 1\n",
    "\n",
    "        self.vocabulary = list(self.word_counts.keys())\n",
    "        self.prefix_tree = PrefixTree(self.vocabulary)\n",
    "\n",
    "    def get_words_and_probs(self, prefix: str) -> (List[str], List[float]):\n",
    "        \"\"\"\n",
    "        Возвращает список слов, начинающихся на prefix,\n",
    "        с их вероятностями (нормировать ничего не нужно)\n",
    "        \"\"\"\n",
    "        words = self.prefix_tree.search_prefix(prefix)\n",
    "        probs = [self.word_counts[word] / self.total_words for word in words]\n",
    "        return words, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_corpus = [\n",
    "    [\"aa\", \"ab\"],\n",
    "    [\"aaa\", \"abab\"],\n",
    "    [\"abb\", \"aa\", \"ab\", \"bba\", \"bbb\", \"bcd\"],\n",
    "]\n",
    "\n",
    "word_completor = WordCompletor(dummy_corpus)\n",
    "words, probs = word_completor.get_words_and_probs('a')\n",
    "words_probs = list(zip(words, probs))\n",
    "assert set(words_probs) == {('aa', 0.2), ('ab', 0.2), ('aaa', 0.1), ('abab', 0.1), ('abb', 0.1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предсказание следующих слов\n",
    "\n",
    "Теперь, когда мы умеем дописывать слово за пользователем, мы можем пойти дальше и предожить ему несколько следующих слов с учетом дописанного. Для этого мы воспользуемся n-граммами и будем советовать n следующих слов. Но сперва нужно получить n-граммную модель.\n",
    "\n",
    "Напомним, что вероятность последовательности для такой модели записывается по формуле\n",
    "$$\n",
    "P(w_1, \\dots, w_T) = \\prod_{i=1}^T P(w_i \\mid w_{i-1}, \\dots, w_{i-n}).\n",
    "$$\n",
    "\n",
    "Тогда, нам нужно оценить $P(w_i \\mid w_{i-1}, \\dots, w_{i-n})$ по частоте встречаемости n-граммы.   \n",
    "\n",
    "__Задание 4 (1 балл).__ Напишите класс для n-граммной модели. Понятное дело, никакого сглаживания добавлять не надо, мы же не хотим, чтобы модель советовала случайные слова (хоть и очень редко)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class NGramLanguageModel:\n",
    "    def __init__(self, corpus: List[List[str]], n: int):\n",
    "        self.n = n\n",
    "        self.ngram_counts = Counter()\n",
    "        self.context_counts = Counter()\n",
    "\n",
    "        for sentence in corpus:\n",
    "            length = len(sentence)\n",
    "            for i in range(length):\n",
    "                for k in range(1, length - i + 1):\n",
    "                    ngram = tuple(sentence[i:i+k])\n",
    "                    self.ngram_counts[ngram] += 1\n",
    "                    if len(ngram) > 1:\n",
    "                        context = ngram[:-1]\n",
    "                        self.context_counts[context] += 1\n",
    "\n",
    "    def get_next_words_and_probs(self, prefix: list) -> (List[str], List[float]):\n",
    "        \"\"\"\n",
    "        Возвращает список слов, которые могут идти после prefix,\n",
    "        а так же список вероятностей этих слов\n",
    "        \"\"\"\n",
    "\n",
    "        prefix = tuple(prefix)\n",
    "        total_count = self.context_counts.get(prefix, 0)\n",
    "        if total_count == 0:\n",
    "            return [], []\n",
    "\n",
    "        candidates = {}\n",
    "        for ngram in self.ngram_counts:\n",
    "            if len(ngram) == len(prefix) + 1 and ngram[:-1] == prefix:\n",
    "                next_word = ngram[-1]\n",
    "                candidates[next_word] = self.ngram_counts[ngram]\n",
    "\n",
    "        next_words = []\n",
    "        probs = []\n",
    "        for word, count in candidates.items():\n",
    "            prob = count / total_count\n",
    "            next_words.append(word)\n",
    "            probs.append(prob)\n",
    "\n",
    "        return next_words, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_corpus = [\n",
    "    ['aa', 'aa', 'aa', 'aa', 'ab'],\n",
    "    ['aaa', 'abab'],\n",
    "    ['abb', 'aa', 'ab', 'bba', 'bbb', 'bcd']\n",
    "]\n",
    "\n",
    "n_gram_model = NGramLanguageModel(corpus=dummy_corpus, n=2)\n",
    "\n",
    "next_words, probs = n_gram_model.get_next_words_and_probs(['aa', 'aa'])\n",
    "words_probs = list(zip(next_words, probs))\n",
    "\n",
    "assert set(words_probs) == {('aa', 2/3), ('ab', 1/3)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично, мы теперь можем объединить два метода в автоматический дописыватель текстов: первый будет дополнять слово, а второй – предлагать продолжения. Хочется, чтобы предлагался список возможных продолжений, из который пользователь сможет выбрать наиболее подходящее. Самое сложное тут – аккуратно выбирать, что показывать, а что нет.   \n",
    "\n",
    "__Задание 5 (1 балл).__ В качестве первого подхода к снаряду реализуйте метод, возвращающий всегда самое вероятное продолжение жадным способом. Если вы справитесь, то сможете можете добавить опцию поддержки нескольких вариантов продолжений, что сделает метод гораздо лучше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "\n",
    "class TextSuggestion:\n",
    "    def __init__(self, word_completor, n_gram_model):\n",
    "        self.word_completor = word_completor\n",
    "        self.n_gram_model = n_gram_model\n",
    "\n",
    "    def suggest_text(self, text: Union[str, list], n_words=3, n_texts=1) -> list[list[str]]:\n",
    "        \"\"\"\n",
    "        Возвращает возможные варианты продолжения текста (по умолчанию только один)\n",
    "        \n",
    "        text: строка или список слов – написанный пользователем текст\n",
    "        n_words: число слов, которые дописывает n-граммная модель\n",
    "        n_texts: число возвращаемых продолжений (пока что только одно)\n",
    "        \n",
    "        return: list[list[srt]] – список из n_texts списков слов, по 1 + n_words слов в каждом\n",
    "        Первое слово – это то, которое WordCompletor дополнил до целого.\n",
    "        \"\"\"\n",
    "\n",
    "        suggestions = []\n",
    "\n",
    "        if isinstance(text, str):\n",
    "            words = text.strip().split()\n",
    "        else:\n",
    "            words = text[:]\n",
    "        \n",
    "        if not words:\n",
    "            return []\n",
    "        \n",
    "        last_word = words[-1]\n",
    "        completions, probs = self.word_completor.get_words_and_probs(last_word)\n",
    "        if completions:\n",
    "            max_prob_index = probs.index(max(probs))\n",
    "            completed_word = completions[max_prob_index]\n",
    "        else:\n",
    "            completed_word = last_word\n",
    "        \n",
    "\n",
    "        full_words = words[:-1] + [completed_word]\n",
    "        suggestion = [completed_word]\n",
    "        n = self.n_gram_model.n\n",
    "        if n > 1:\n",
    "            context = full_words[-(n - 1):]\n",
    "        else:\n",
    "            context = []\n",
    "        for _ in range(n_words):\n",
    "            next_words, next_probs = self.n_gram_model.get_next_words_and_probs(context)\n",
    "            if not next_words:\n",
    "                break\n",
    "            max_prob_index = next_probs.index(max(next_probs))\n",
    "            next_word = next_words[max_prob_index]\n",
    "            suggestion.append(next_word)\n",
    "            if n > 1:\n",
    "                context = context[1:] + [next_word]\n",
    "            else:\n",
    "                context = [next_word]\n",
    "        \n",
    "        suggestions.append(suggestion)\n",
    "        return suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_corpus = [\n",
    "    ['aa', 'aa', 'aa', 'aa', 'ab'],\n",
    "    ['aaa', 'abab'],\n",
    "    ['abb', 'aa', 'ab', 'bba', 'bbb', 'bcd']\n",
    "]\n",
    "\n",
    "word_completor = WordCompletor(dummy_corpus)\n",
    "n_gram_model = NGramLanguageModel(corpus=dummy_corpus, n=2)\n",
    "text_suggestion = TextSuggestion(word_completor, n_gram_model)\n",
    "\n",
    "assert text_suggestion.suggest_text(['aa', 'aa'], n_words=3, n_texts=1) == [['aa', 'aa', 'aa', 'aa']]\n",
    "assert text_suggestion.suggest_text(['abb', 'aa', 'ab'], n_words=2, n_texts=1) == [['ab', 'bba', 'bbb']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_suggestion = TextSuggestion(word_completor, n_gram_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Настало время довести вашу систему до ума. В этой части вы можете модифицировать все классы по своему усмотрению и добавлять любые эвристики. Если нужно, то дополнительно обрабатывать текст и вообще делать все, что считаете нужным, __кроме использования дополнительных данных__. Главное – вы должны обернуть вашу систему в пользовательский интерфейс с помощью [reflex](https://github.com/reflex-dev/reflex). В нем можно реализовать почти любой функционал по вашему желанию.\n",
    "\n",
    "Мы настоятельно рекомендуем вам оформить код в проект, а не писать в ноутбуке. Но если вам очень хочется писать тут, то хотя бы не меняйте код в предыдущих заданиях, чтобы его можно было нормально оценивать.\n",
    "\n",
    "При сдаче решения прикрепите весь ваш __код__, __отчет__ по второй части и __видео__ с демонстрацией работы вашей системы. Удачи!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "notebookId": "53997d2d-afb8-4477-8874-b6d46299f06c",
  "notebookPath": "seminar.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
