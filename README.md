# text_suggestion

<img src="/suggestion app video.gif" alt="project"/>


### Отчет по созданию системы автодополнения текста и предсказания слов

### Общая цель проекта

Целью разработки было создание системы для автодополнения текста и предсказания следующих слов на основе частотного анализа и n-граммной модели. Система была реализована с использованием Python и Reflex для интеграции пользовательского интерфейса.

---

## 1. Создание префиксного дерева (`PrefixTree`)

### Цели

1. Эффективный поиск слов по заданному префиксу.
2. Быстрая вставка новых слов.

### Реализованные методы

- `Insert`:
    - Добавляет слова в дерево, создавая ветви для каждого символа.
    - Формирует структуру, обеспечивающую доступ за O(n), где n — длина слова.
- `search_prefix`:
    - Находит узел, соответствующий последнему символу префикса.
    - Возвращает указатель на этот узел для последующего обхода.
- `_dfs` (обход в глубину):
    - Используется для поиска всех слов, начинающихся на указанный префикс.
    - Добавляет слова в результирующий список.
    - Сложность поиска — O(m), где m — количество подходящих слов.

### Особенности

- Префиксное дерево обеспечивает скорость поиска, подходящую для работы с большими корпусами текста.
- Легкость добавления новых слов и расширения структуры.

---

## 2. Сбор частотности слов

### Техники обработки данных

1. Подсчет частотности слов:
    - Использован defaultdict(int) для подсчета количества вхождений каждого слова.
    - Частота хранится в виде пары слово: частота.
2. Фильтрация данных:
    - Редкие слова (с частотой ниже порога) исключены для улучшения производительности.
    - Очищены данные от шумов (символов, не входящих в алфавит).
3. Общая частота:
    - Общая сумма всех слов сохраняется для вычисления вероятностей.

### Связь с префиксным деревом

- Частоты слов передаются для построения PrefixTree.
- Используются в методе get_words_and_probs для вычисления вероятностей слов.

---

## 3. Построение n-граммной модели

### Цели

1. Моделирование контекста текста для предсказания следующего слова.
2. Генерация текстов, основанных на частотах n-грамм.

### Сбор n-грамм

- Из корпуса текста генерируются n-граммы всех возможных длин.
- Используется collections.Counter для подсчета частот n-грамм и их контекстов (всех слов, кроме последнего).

### Методы

- `get_next_words_and_probs`:
    - Находит n-граммы, соответствующие заданному контексту.
    - Вычисляет вероятности следующих слов как отношение частоты n-граммы к частоте контекста.
    - Возвращает список слов и их вероятности.

---

## 4. Генерация текстовых предложений

### Класс `TextSuggestion`

Объединяет WordCompletor и NGramLanguageModel для автодополнения текста и генерации следующих слов.

### Алгоритм работы

1. Обработка ввода:
    - Текст разбивается на слова.
    - Последнее слово обрабатывается через WordCompletor для автодополнения.
2. Генерация следующих слов:
    - Используется n-граммная модель для предсказания следующего слова.
    - Процесс продолжается до достижения заданного числа слов или отсутствия предсказаний.
3. Вывод:
    - Возвращает сгенерированное предложение, дополненное наиболее вероятными словами.

---

## 5. Интеграция с Reflex

### Цели

1. Создать интуитивный интерфейс для взаимодействия с системой.
2. Обеспечить мгновенное обновление интерфейса при изменении состояния.

### Реализация

### Состояние

- Переменные:
    - user_input: ввод пользователя.
    - suggestions: список предложений для автодополнения.
    - predicted_text: сгенерированный текст.
- Методы:
    - update_suggestions: обновляет список предложений на основе текущего ввода.
    - select_suggestion: заменяет последнее слово выбранным предложением.

### Интерфейс

- Поле ввода для текста.
- Кнопки с предложениями для выбора.
- Область отображения сгенерированного текста.

---

## 6. Результаты и оптимизация

### Достижения:

1. Высокая точность предсказаний.
2. Эффективное использование структуры данных (`defaultdict`, Counter, `PrefixTree`).
3. Удобный интерфейс для пользователей.

### Что не удалось:

- Из-за недостатка мощностей компьютера, не удалось обучить модель на большом количестве данных.

### Оптимизация:

1. Удаление нерелевантных слов для экономии памяти.
2. Использование сглаживания для работы с редкими контекстами.
3. Возможность добавления альтернативных моделей.

---
...
